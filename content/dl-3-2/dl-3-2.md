---
title: "吴恩达深度学习观后感第三课(2): ML Strategy(2)"
date: 2018-09-02
---

这一周还是讲ML Strategy，主要内容是错误分析，不匹配的训练集和开发/测试集，从多任务中学习，端到端的深度学习，学习目标是：

- Understand what multi-task learning and transfer learning are
- Recognize bias, variance and data-mismatch by looking at the performances of your algorithm on train/dev/test sets

## 错误分析

为了使模型变得更好，我们需要分析错误分类的例子，造成错误的原因有很多，可以数一下每种错误占的比例，来决定先解决哪一种问题，因为如果某一种错误占的比例很小，那么并不值得优先去花大力气。

假设在训练集上有一些错误标注的数据，深度学习算法对训练集上的随机错误是健壮的（robust），但对系统错误就不是那么健壮了。在进行错误分析时，如果由错误标注导致的错误占比较多，那么应该考虑改正这些错误标注，开发集和测试集都要改，另外，正确的预测中也可能会包含错误的标注，也可以一看，不过数据量较大，因为通常模型的正确率会比较高。训练集由于数据量太大，很难人工去修改，现在训练集和开发/测试集的分布不一致了，实际上，轻微的不一致是可以接受的。

做机器学习的要诀之一就是快速做出第一个系统，快速迭代。

## 不匹配的训练集和开发/测试集

深度学习需要大量数据，但是没有那么多和开发/测试集分布一致的数据作为训练集，这些数据也是可以使用的，那么如何在这种情况下进行训练呢？我们可以把开发/测试集划出一部分数据，合入训练集，再从训练集中划出一小块作为训练开发集（training-dev set）。

在新的集合设置上，重新明确一下bias和variance，human level error (avoidable bias) training set error (variance) training-dev set error (data mismatch) Dev set error。

解决数据不匹配的问题，首先要人工进行错误分析，理解训练集和开发集的数据的区别是什么，然后努力使训练集变的和开发集相似，或者说收集更多的相似的数据作为训练集。为了获得更多的数据，可以人工合成数据，要注意不要合成出来的数据只是所有可能数据的很小的一个子集。

## 从多任务中学习

迁移学习了解一下，对于相似的任务不需要从头学习，可以利用已经训练的网络模型。什么时候可以用迁移学习：

- Task A and B have the same input x.
- You have a lot more data for Task A than Task B.
- Low level features from A could be helpful for learning B.

再了解一下多任务学习，就是多个任务同时学习，比如自动驾驶中，判断一张图片中有没有行人，有没有车，有没有交通标志。什么时候可以用多任务学习：

- Training on a set of tasks that could benefit from having shared lower-level features.
- Usually: Amount of data you have for each task is quite similar.
- Can train a big enough neural network to do well on all the tasks.

比起多任务学习，迁移学习更常用一点。

## 端到端的深度学习

端到端的深度学习在有足够的数据的情况下，可以表现得很好，可以简化系统设计，但是端到端的深度学习也不是万能的。它的优点有：

- Let the data speak
- Less hand-designing of components needed

缺点包括：

- May need large amount of data
- Excludes potentially useful hand-designed components

是否使用端到端的深度学习的关键问题是：Do you have sufficient data to learn a function of the complexity needed to map x to y?
